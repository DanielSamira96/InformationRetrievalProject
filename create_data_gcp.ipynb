{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"a00e032c"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"5ac36d3a","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-369f  GCE       4                                       RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"51cf86c5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"d8f56ecd","metadata":{"id":"d8f56ecd","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import math\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"38a897f2","metadata":{"id":"38a897f2","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Dec 28 10:26 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"72bed56b","metadata":{"id":"72bed56b","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-369f-m.c.myprojectds-370114.internal:40529\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fa57469c520>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":7,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'bucket316164417' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name.endswith('.parquet'):\n","        paths.append(full_path+b.name)\n","        \n","bucket_name = 'projectdata316164417'"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"cac891c2"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"0d7e2971"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"701811af"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":8,"id":"121fe102","metadata":{"id":"121fe102","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":9,"id":"57c101a8","metadata":{"id":"57c101a8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":10,"id":"c259c402","metadata":{"id":"c259c402"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"481f2044"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":11,"id":"e4c523e7","metadata":{"id":"e4c523e7","outputId":"8c4258e9-7ed8-4076-e727-b41cd11b8401","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)"]},{"cell_type":"code","execution_count":12,"id":"82881fbf","metadata":{"id":"82881fbf","outputId":"5e62dd02-3595-459f-e1ad-cb49eb92c341"},"outputs":[],"source":["# Count number of wiki pages\n","#parquetFile.count()"]},{"cell_type":"code","execution_count":13,"id":"f3ad8fea","metadata":{"id":"f3ad8fea","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def word_count(id, text):\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  cnt = Counter()\n","  for token in tokens:\n","    if token not in all_stopwords:\n","      cnt[token] += 1\n","  return [(token,(id, val)) for token, val in cnt.items()]\n","\n","def reduce_word_counts(unsorted_pl):\n","  return sorted(unsorted_pl)\n","\n","def reduce_dict(unsorted_pl):\n","  return sorted(set(unsorted_pl))\n","\n","def calculate_df(postings):\n","  return postings.map(lambda x: (x[0],len(x[1])))\n","\n","def partition_postings_and_write(postings, base_dir):\n","  return postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey().map(lambda x: InvertedIndex.write_a_posting_list(x, base_dir, bucket_name))\n","\n","def partition_postings_and_write_anchor(postings, base_dir):\n","  return postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey().map(lambda x: InvertedIndex.write_a_posting_list_anchor(x, base_dir, bucket_name))"]},{"cell_type":"code","execution_count":14,"id":"Xusg1VF28eEO","metadata":{"id":"Xusg1VF28eEO"},"outputs":[],"source":["def tokens_each_doc(text):\n","  return [token.group() for token in RE_WORD.finditer(text.lower()) if token not in all_stopwords]\n","\n","def DL_nfi(posting):\n","  totalLength = 0\n","  totalPow2 = 0\n","  for term, tf in posting:\n","    totalLength += tf\n","    totalPow2 += math.pow(tf, 2)\n","  return (totalLength, 1 / math.sqrt(totalPow2))\n","\n","def anchor_tokenizing(lst):\n","  tokens = []\n","  for item in lst:\n","    tokens.extend([(token.group(), item[0]) for token in RE_WORD.finditer(item[1].lower()) if token not in all_stopwords])\n","  return tokens"]},{"cell_type":"markdown","id":"etXLnVm67kj-","metadata":{"id":"etXLnVm67kj-"},"source":["**TITLE:**\n"]},{"cell_type":"code","execution_count":15,"id":"qHOESUU_7lCW","metadata":{"id":"qHOESUU_7lCW"},"outputs":[],"source":["doc_pairs_title = parquetFile.select(\"id\", \"title\").rdd\n","doc_to_title = doc_pairs_title.collectAsMap()\n","DL_title = doc_pairs_title.map(lambda x: (x[0], len(tokens_each_doc(x[1])))).collectAsMap()\n","word_counts_title = doc_pairs_title.flatMap(lambda x: word_count(x[0], x[1]))\n","postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n","# postings_filtered_title = postings_title.filter(lambda x: len(x[1])>50)\n","w2df_title = calculate_df(postings_title)\n","w2df_dict_title = w2df_title.collectAsMap()\n","posting_locs_list_title = partition_postings_and_write(postings_title, 'title_index').collect()"]},{"cell_type":"code","execution_count":16,"id":"S2Fq1cY5-7aX","metadata":{"id":"S2Fq1cY5-7aX"},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_title = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title_index'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs_title[k].extend(v)\n","\n","# Create inverted index instance\n","inverted_title = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted_title.posting_locs = super_posting_locs_title\n","# Add the token - df dictionary to the inverted index\n","inverted_title.df = w2df_dict_title\n","inverted_title.DL = DL_title\n","inverted_title.N = len(DL_title)\n","total_val = 0.0\n","for x in DL_title.values():\n","  total_val += x\n","inverted_title.avgdl = total_val / inverted_title.N\n","# write the global stats out\n","inverted_title.write_index('.', 'title')\n","# upload to gs\n","index_src = \"title.pkl\"\n","index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":17,"id":"PUN1sgFYNKdv","metadata":{"id":"PUN1sgFYNKdv"},"outputs":[],"source":["with open(Path('.') / f'doc_to_title.pkl', 'wb') as f:\n","    pickle.dump(doc_to_title, f)\n","src = \"doc_to_title.pkl\"\n","dst = f'gs://{bucket_name}/{src}'\n","!gsutil cp $src $dst"]},{"cell_type":"markdown","id":"jQ699XXI65Xw","metadata":{"id":"jQ699XXI65Xw"},"source":["**BODY:**"]},{"cell_type":"code","execution_count":18,"id":"2NW26J5x6vNf","metadata":{"id":"2NW26J5x6vNf"},"outputs":[],"source":["doc_pairs_text = parquetFile.select(\"id\", \"text\").rdd"]},{"cell_type":"code","execution_count":19,"id":"380832f9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# DL_text = doc_pairs_text.map(lambda x: (x[0], DL_nfi(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":20,"id":"8ea7d7e2","metadata":{},"outputs":[],"source":["word_counts_text = doc_pairs_text.flatMap(lambda x: word_count(x[0], x[1]))"]},{"cell_type":"code","execution_count":21,"id":"d8a0b3eb","metadata":{},"outputs":[],"source":["postings_text = word_counts_text.groupByKey().mapValues(reduce_word_counts)\n","postings_filtered_text = postings_text.filter(lambda x: len(x[1])>50)"]},{"cell_type":"code","execution_count":null,"id":"410d4048","metadata":{},"outputs":[],"source":["DL_text = postings_filtered_text.flatMap(lambda x: [(tup[0], (x[0], tup[1])) for tup in x[1]]).groupByKey().mapValues(list).map(lambda x: (x[0], DL_nfi(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":22,"id":"c3b2e4ad","metadata":{},"outputs":[],"source":["w2df_text = calculate_df(postings_filtered_text)"]},{"cell_type":"code","execution_count":null,"id":"6f16086d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["w2df_dict_text = w2df_text.collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"6414dc28","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["posting_locs_list_text = partition_postings_and_write(postings_filtered_text, 'body_index').collect()"]},{"cell_type":"code","execution_count":null,"id":"20ErssPq-6Qe","metadata":{"id":"20ErssPq-6Qe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://body.pkl [Content-Type=application/octet-stream]...\n","| [1 files][129.9 MiB/129.9 MiB]                                                \n","Operation completed over 1 objects/129.9 MiB.                                    \n"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_text = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='body_index'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs_text[k].extend(v)\n","\n","# Create inverted index instance\n","inverted_text = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted_text.posting_locs = super_posting_locs_text\n","# Add the token - df dictionary to the inverted index\n","inverted_text.df = w2df_dict_text\n","inverted_text.DL = DL_text\n","inverted_text.N = len(DL_text)\n","total_val = 0.0\n","for x in DL_text.values():\n","  total_val += x[0]\n","inverted_text.avgdl = total_val / inverted_text.N\n","# write the global stats out\n","inverted_text.write_index('.', 'body')\n","# upload to gs\n","index_src = \"body.pkl\"\n","index_dst = f'gs://{bucket_name}/body_index/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"markdown","id":"SEoYi2Gr7z0W","metadata":{"id":"SEoYi2Gr7z0W"},"source":["**ANCHOR:**\n"]},{"cell_type":"code","execution_count":27,"id":"HSmVQ95T70KW","metadata":{"id":"HSmVQ95T70KW"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["doc_pairs_anchor = parquetFile.select(\"id\", \"anchor_text\").rdd\n","word_counts_anchor = doc_pairs_anchor.flatMap(lambda x: anchor_tokenizing(x[1]))\n","postings_anchor = word_counts_anchor.groupByKey().mapValues(reduce_dict)\n","# postings_filtered_anchor = postings_anchor.filter(lambda x: len(x[1])>50)\n","w2df_anchor = calculate_df(postings_anchor)\n","w2df_dict_anchor = w2df_anchor.collectAsMap()\n","posting_locs_list_anchor = partition_postings_and_write_anchor(postings_anchor, 'anchor_index').collect()"]},{"cell_type":"code","execution_count":28,"id":"A-Zb4j_r-78T","metadata":{"id":"A-Zb4j_r-78T"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://anchor.pkl [Content-Type=application/octet-stream]...\n","\\ [1 files][ 95.0 MiB/ 95.0 MiB]                                                \n","Operation completed over 1 objects/95.0 MiB.                                     \n"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_anchor = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='anchor_index'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs_anchor[k].extend(v)\n","\n","# Create inverted index instance\n","inverted_anchor = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted_anchor.posting_locs = super_posting_locs_anchor\n","# Add the token - df dictionary to the inverted index\n","inverted_anchor.df = w2df_dict_anchor\n","# write the global stats out\n","inverted_anchor.write_index('.', 'anchor')\n","# upload to gs\n","index_src = \"anchor.pkl\"\n","index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":29,"id":"8f880d59","metadata":{"id":"8f880d59","nbgrader":{"grade":false,"grade_id":"cell-index_dst_size","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cdfd13d8-068b-479b-fc3b-5d639624d051"},"outputs":[{"name":"stdout","output_type":"stream","text":[" 95.02 MiB  2022-12-28T15:30:08Z  gs://bucket316164417/anchor_index/anchor.pkl\r\n","TOTAL: 1 objects, 99632460 bytes (95.02 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"c52dee14","metadata":{"id":"c52dee14","nbgrader":{"grade":false,"grade_id":"cell-2a6d655c112e79c5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# PageRank"]},{"cell_type":"code","execution_count":30,"id":"31a516e2","metadata":{"id":"31a516e2"},"outputs":[],"source":["# Put your `generate_graph` function here\n","def generate_graph(pages):\n","  edges = pages.flatMap(lambda x: set([(x[0], y[0]) for y in x[1]]))\n","  vertices = pages.flatMap(lambda x: [(x[0], )] + [(y[0],) for y in x[1]]).distinct()\n","  return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"6bc05ba3","metadata":{"id":"6bc05ba3","nbgrader":{"grade":false,"grade_id":"cell-PageRank","locked":false,"schema_version":3,"solution":true,"task":false},"outputId":"2762685f-40f1-4cee-a2d6-abfb0ab89a2c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 321:==================================================>  (192 + 8) / 200]\r"]},{"name":"stdout","output_type":"stream","text":["+---+------------------+\n","| id|          pagerank|\n","+---+------------------+\n","| 12| 147.7334534423781|\n","| 25| 59.46853992954965|\n","| 39| 57.71570853205964|\n","|290|31.598711370141082|\n","|303|361.74497456349076|\n","|305| 33.85560357229939|\n","|307| 330.1845085435861|\n","|308|387.29475534179966|\n","|309|5.7470679397511635|\n","|316| 38.02341498210656|\n","|324| 512.0423004762725|\n","|330|  0.26680664641919|\n","|332| 4.111818467779909|\n","|334|21.560735631698087|\n","|336| 31.28590516741786|\n","|339| 36.05089151175037|\n","|340| 3.351263223337941|\n","|344| 6.885160497491854|\n","|358|484.09636397351534|\n","|359|0.6129794446734336|\n","+---+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(doc_pairs_anchor)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('id'))\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr.show()"]},{"cell_type":"code","execution_count":null,"id":"f7717604","metadata":{"id":"f7717604","nbgrader":{"grade":true,"grade_id":"cell-PageRank_time","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying file://pagerank.pkl [Content-Type=application/octet-stream]...\n","\\ [1 files][ 84.7 MiB/ 84.7 MiB]                                                \n","Operation completed over 1 objects/84.7 MiB.                                     \n"]}],"source":["with open(Path('.') / f'pagerank.pkl', 'wb') as f:\n","    pickle.dump(pr.toPandas().set_index('id')['pagerank'].to_dict(), f)\n","src = \"pagerank.pkl\"\n","dst = f'gs://{bucket_name}/{src}'\n","!gsutil cp $src $dst"]},{"cell_type":"code","execution_count":null,"id":"19411347","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-12-28 16:05:52--  https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2\n","Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142, 2620:0:861:2:208:80:154:142\n","Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2503235912 (2.3G) [application/octet-stream]\n","Saving to: â€˜pageviews-202108-user.bz2â€™\n","\n","pageviews-202108-us 100%[===================>]   2.33G  4.67MB/s    in 8m 45s  \n","\n","2022-12-28 16:14:37 (4.55 MB/s) - â€˜pageviews-202108-user.bz2â€™ saved [2503235912/2503235912]\n","\n"]}],"source":["# Paths\n","# Using user page views (as opposed to spiders and automated traffic) for the \n","# month of August 2021\n","pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path) \n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","# Download the file (2.3GB) \n","!wget -N $pv_path\n","# Filter for English pages, and keep just two fields: article ID (3) and monthly \n","# total number of page views (5). Then, remove lines with article id or page \n","# view values that are not a sequence of digits.\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","# Create a Counter (dictionary) that sums up the pages views for the same \n","# article, resulting in a mapping from article id to total page views.\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","  for line in f:\n","    parts = line.split(' ')\n","    wid2pv.update({int(parts[0]): int(parts[1])})"]},{"cell_type":"code","execution_count":null,"id":"1140556d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://pageviews-202108-user.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 73.5 MiB/ 73.5 MiB]                                                \n","Operation completed over 1 objects/73.5 MiB.                                     \n"]}],"source":["# write out the counter as binary file (pickle it)\n","with open(pv_clean, 'wb') as f:\n","  pickle.dump(wid2pv, f)\n","src = pv_clean\n","dst = f'gs://{bucket_name}/{src}'\n","!gsutil cp $src $dst"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}
